# -*- coding: utf-8 -*-
"""emotions analysis(Pipeline).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EuaOtLBMytGoWRTzXIyVZs3MdfJD2mCC

### 1.Problem Statement
#### Create a model for predict that coment/text belongs to which type of emotion (fear/angry/joy)
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
from textblob import TextBlob as tb
from autocorrect import Speller as sp
import emoji
import nltk
from nltk.tokenize import word_tokenize as wt,sent_tokenize as st
from nltk.corpus import stopwords
from nltk import PorterStemmer,LancasterStemmer,SnowballStemmer
from nltk import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer

from sklearn.pipeline import Pipeline ,make_pipeline
from sklearn.model_selection import train_test_split

from sklearn.preprocessing import FunctionTransformer,PowerTransformer

pip install autocorrect

pip install emoji

pip install nltk

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

"""### 2.Data Collection"""

# take a data set from kaggle which used to classify emotions based on text/comments.
# Convert column names correctly as mensioned in problem statement.
data = pd.read_csv(r"/content/drive/MyDrive/Emotion_classify_Data.csv")
data.head()

"""### 3.EDA Before pre-processing"""

data.shape   # data contains 5937 rows and 2 columns

data.info()

data.isnull().sum()     # in this data doesn't have any missing values

data.duplicated().sum()     # this data doesn't have any duplicate values

data.Emotion.unique()   # unique values for class label of emotions

data.Emotion.value_counts() # 2000 rows belongs to anger , 2000 rows belongs to joy and 1937 rows belongs to fear

# as per my problem statement i don't want some columns,beacuase there is no use for those columns for solve my problem statemnet
# split my data into featue variables and class variables
fv = data.iloc[:,0]
cv = data.iloc[:,-1]

fv.head()

cv= cv.map({"anger":0,"joy":2,"fear":1})    # Covert class labels anger as 0 , joy as 2 and fear as 1

cv.head()

sns.countplot(data.Emotion)
plt.show()

# split my data into x_train and x_test
x_train,x_test,y_train,y_test = train_test_split(fv,cv,test_size=0.2,stratify=cv)

x_train.shape     # x_train data contains 4749 rows

x_test.shape      # x_test data contains 1188 rows

"""### 4. Pre-processing"""

# basic pre=processing steps

def lowers(x):
    return x.str.lower()

def html(x):
    return x.apply(lambda x:re.sub("<.+?>"," ",x))

def url(x):
    return x.apply(lambda x:re.sub("https[s]?://.+? +"," ",x))

def unw(x):
    return x.apply(lambda x:re.sub("[]()*\-.:,@#$%&^!?/0-9']"," ",x))

def emoji_remove(x):

    x = x.apply(lambda x : emoji.demojize(x))

    return x

def lemma(x):
  list_stp = stopwords.words("english")  # list_stp contains group of stop words.
  wl=WordNetLemmatizer()

  def lemmatize_text(text):
        words = wt(text)
        lemmatized_words = [wl.lemmatize(word, pos="v") for word in words if word not in list_stp]
        return " ".join(lemmatized_words)

  return x.apply(lemmatize_text)

lemma(fv)

# create a pipeline for pre-processing the data
pre_pro_pi = Pipeline([("Emoji's", FunctionTransformer(emoji_remove)),("lower",FunctionTransformer(lowers)),("html",FunctionTransformer(html)),
                       ("url",FunctionTransformer(url)),("unw",FunctionTransformer(unw)),("advance",FunctionTransformer(lemma))])

pre_pro_pi

final_pipe = Pipeline([("pre_process",pre_pro_pi),("vectorizer",CountVectorizer())])

final_pipe.fit_transform(x_train)

import pickle
pickle.dump(final_pipe,open(r"/content/drive/MyDrive/emotions_pre1.pkl","wb"))



"""### 5.EDA after pre-processing
we are perform on text data,so this is totally based on frequency of words or frequency of chracters or most common words based on types of Targets
"""

xtrain_pre = pre_pro_pi.fit_transform(x_train)

featur_var = pd.DataFrame(xtrain_pre,columns=["Comment"])
class_var = pd.DataFrame(y_train,columns = ["Emotion"])

df = pd.concat([featur_var,class_var],axis=1)    # concat fv and data["Target"] as new data frame
df.head()

df["Emotion"] = df["Emotion"].map({0:"anger",2:"joy",1:"fear"})

df.head()

# seperate data into Targets.
gp = df.groupby("Emotion")
anger = gp.get_group("anger")["Comment"]
fear = gp.get_group("joy")["Comment"]
joy = gp.get_group("fear")["Comment"]

gp.get_group("anger")["Comment"]

# Combine entire rows text into one text
ft = " ".join(gp.get_group("anger")["Comment"]).split()
jt = " ".join(gp.get_group("joy")["Comment"]).split()
at = " ".join(gp.get_group("fear")["Comment"]).split()

# find how many words contain each emotion
print("anger class contains",len(ft),"number of words")
print("joy class contains",len(jt),"number of words")
print("fear class contains",len(at),"number of words")

"""### find top most common words in each class"""

pip install wordcloud

from collections import Counter
Counter(ft).most_common(10)

from wordcloud import WordCloud
wc = WordCloud().generate(" ".join(gp.get_group("anger")["Comment"]))
plt.imshow(wc)
plt.show()

Counter(jt).most_common(10)

from wordcloud import WordCloud
wc = WordCloud().generate(" ".join(gp.get_group("joy")["Comment"]))
plt.imshow(wc)
plt.show()

Counter(at).most_common(10)

from wordcloud import WordCloud
wc = WordCloud().generate(" ".join(gp.get_group("fear")["Comment"]))
plt.imshow(wc)
plt.show()

"""### 6.Feature engineering"""

xtest_pre = pre_pro_pi.transform(x_test)
cv = CountVectorizer()    # it will take CountVectorizer function

"""### 7.Training"""

from sklearn.naive_bayes import BernoulliNB,MultinomialNB,CategoricalNB

final_xtrain = final_pipe.fit_transform(x_train)
final_xtest = final_pipe.transform(x_test)

mb = MultinomialNB()
model = mb.fit(final_xtrain,y_train)

(final_xtrain)

pickle.dump(model,open(r"/content/drive/MyDrive/emotion_model1.pkl","wb"))

"""### 8.Model Evalution"""

from sklearn.metrics import accuracy_score , classification_report,confusion_matrix

accuracy_score(y_test,model.predict(final_xtest))  # By using multinominal it will give 90% of accuracy rate

print(classification_report(y_test,model.predict(final_pipe.transform(x_test))) )



"""### 9.Testing"""

m = final_pipe.transform(pd.Series("i am very happy today.because i got job at top mnc company"))
pred = model.predict(m)   # it is used to shows that predicted value

if pred == 0:
    print("anger")
elif pred == 1:
  print("fear")
else:
    print("joy")

"""finally I send a text as querry to the model,then it will give the result as real emotion of that text is "JOY"
"""

