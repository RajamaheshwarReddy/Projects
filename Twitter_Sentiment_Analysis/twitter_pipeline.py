# -*- coding: utf-8 -*-
"""Twitter_Pipeline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z8FKHrvDH8raQbNnA0TZuIC3IezAjS9H

**Problem statement**

#### Preforming the sentiment analysis on tweets in the twitter.

**1.Importing libraries**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
import nltk
import re
import emoji
from collections import Counter
from wordcloud import WordCloud
import pickle
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize,sent_tokenize
from nltk.stem import PorterStemmer,LancasterStemmer,SnowballStemmer,WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import BernoulliNB,MultinomialNB
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
from sklearn.pipeline import Pipeline,make_pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import FunctionTransformer,PowerTransformer,StandardScaler,LabelEncoder

"""**2. Data Collection**"""

data = pd.read_csv(r"/content/drive/MyDrive/twitter.csv", encoding= 'latin-1',header= None,names= ["Target","UserId","Date","Flag","UserName","Text"])

df = data.copy()

df.head()

"""**3. EDA Before Pre-Processing**"""

df.shape

df.info()

df.isnull().sum()

df.duplicated().sum()

df['Target'].value_counts()

df['Target'] = df['Target'].map({0: 'Negative', 4: 'Positive'})

df['Target'].value_counts()

sns.countplot(data= df, x= 'Target')
plt.show()

feature_var = df['Text']

feature_var.head()

def EDA_1(data):

    case = " ".join(data).islower()
    html = data.apply(lambda x:True if re.search("<.+?>",x) else False).sum()
    url = data.apply(lambda x:True if re.search("http[s]?://.+? +",x) else False).sum()
    unwanted = data.apply(lambda x:True if re.search("[]()*\-.:,@#$%&^!?/0-9']",x) else False).sum()

    count = 0
    for sen in data:
        for char in sen.split():
            if emoji.is_emoji(char):
                count += 1

    if case == False:
        print("Contains non lowercase letters")
    else:
        print('All are in lowercase letters')

    if html > 0:
        print("Contains html tags")
    else:
        print('No html tags')

    if url > 0:
        print("Contains urls")
    else:
        print('No urls')

    if unwanted > 0:
        print("Contains unwanted characters")
    else:
        print('No unwanted characters')

    if count > 0:
        print('Contains Emoji')
    else:
        print('No Emoji')

EDA_1(feature_var)

class_var = df['Target']

class_var.head()

class_var_encoding = LabelEncoder()

class_var = class_var_encoding.fit_transform(class_var)

class_var = pd.Series(class_var)

class_var.head()

x_train,x_test,y_train,y_test = train_test_split(feature_var, class_var, test_size= 0.2, random_state= 1, stratify= class_var)

x_train,x_cv,y_train,y_cv = train_test_split(x_train, y_train, test_size= 0.2, random_state= 1, stratify= y_train)

x_train.head()

"""**4. Pre-Processing**"""

def emoji_remove(x):

    x = x.apply(lambda x : emoji.demojize(x))

    return x

def lowercase(x):

     return x.str.lower()

def html_tags(x):

    x = x.apply(lambda x:re.sub("<.+?>"," ",x))

    return x

def urls(x):

    x = x.apply(lambda x : re.sub("https[s]?://.+? +"," ",x))

    return x

def unwanted_characters(x):

    x = x.apply(lambda x : re.sub("[]()*\-.:,@#$%&^!?/0-9']"," ",x))

    return x

def lemmatization(x):

    list_stp = stopwords.words("english")
    wl = WordNetLemmatizer()

    def lemmatize_text(text):

        words = word_tokenize(text)
        lemmatized_words = [wl.lemmatize(word, pos="v") for word in words if word not in list_stp]

        return " ".join(lemmatized_words)

    return x.apply(lemmatize_text)

preprocesser_pipe = Pipeline([('Lowercase', FunctionTransformer(lowercase)),
                              ('Html_Tags', FunctionTransformer(html_tags)),
                              ('Urls', FunctionTransformer(urls)),
                              ("Emoji's", FunctionTransformer(emoji_remove)),
                              ('Unwanted Characters', FunctionTransformer(unwanted_characters)),
                              ('Lemmatization',FunctionTransformer(lemmatization))])

"""**5. EDA After Pre-Processing**"""

x_train = preprocesser_pipe.fit_transform(x_train)

eda_feature_var = pd.DataFrame(x_train, columns= ['Text'])

eda_class_var = pd.DataFrame(y_train, columns= ['Target'])

eda_df = pd.concat([eda_feature_var, eda_class_var], axis= 1)

eda_df.head()

group_df = eda_df.groupby('Target')

neg_tweets = group_df.get_group(0)['Text']

pos_tweets = group_df.get_group(1)['Text']

neg_tweets_words = ' '.join(neg_tweets).split()

pos_tweets_words = ' '.join(pos_tweets).split()

words = []
freq = []
for i,j in Counter(neg_tweets_words).most_common(7):
    words.append(i)
    freq.append(j)
sns.barplot(x= words, y= freq)
plt.xlabel('Words')
plt.ylabel('Frequency of word')
plt.title('Top 7 common words')
plt.xticks(rotation = 90)
plt.show()

words = []
freq = []
for i,j in Counter(pos_tweets_words).most_common(7):
    words.append(i)
    freq.append(j)
sns.barplot(x= words, y= freq)
plt.xlabel('Words')
plt.ylabel('Frequency of word')
plt.title('Top 7 common words')
plt.xticks(rotation = 90)
plt.show()

wc = WordCloud().generate(' '.join(neg_tweets))
plt.imshow(wc)
plt.show()

wc = WordCloud().generate(' '.join(pos_tweets))
plt.imshow(wc)
plt.show()

"""**6. Feature Engineering**"""

feature_extraction_pipe = Pipeline([('Pre-Processing',preprocesser_pipe),
                                    ('Vectorization',CountVectorizer(binary= True))])

final_x_train = feature_extraction_pipe.fit_transform(x_train)

final_x_cv = feature_extraction_pipe.transform(x_cv)

final_x_test = feature_extraction_pipe.transform(x_test)

pickle.dump(feature_extraction_pipe,open(r"/content/drive/MyDrive/Twitter_feature_extraction.pkl",'wb'))

"""**7. Training & Evalution**"""

for i in range(1,10):
    bernoulli = BernoulliNB(alpha= i)
    model = bernoulli.fit(final_x_train,y_train)
    print(('*'*18 + ' Alpha - {} ' + '*'*18).format(i))
    print(classification_report(y_train, model.predict(final_x_train)))

for i in range(1,10):
    bernoulli = BernoulliNB(alpha= i)
    model = bernoulli.fit(final_x_train,y_train)
    print(('*'*18 + ' Alpha - {} ' + '*'*18).format(i))
    print(classification_report(y_cv, model.predict(final_x_cv)))

"""**8. Testing**"""

bernoulli = BernoulliNB(alpha= 1)
final_model = bernoulli.fit(final_x_train,y_train)

print(classification_report(y_test, model.predict(final_x_test)))

"""**9. Deploying**"""

pickle.dump(final_model,open(r"/content/drive/MyDrive/Twitter_model.pkl",'wb'))

m = feature_extraction_pipe.transform(x_test.iloc[[1]])   ## convert text into sprse matrix used b
pred = final_model.predict(m)                                #it shows predict value

if pred == 0:
  print("Negative")
else:
  print("Positive")