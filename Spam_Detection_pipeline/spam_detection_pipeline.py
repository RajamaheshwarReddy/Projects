# -*- coding: utf-8 -*-
"""Spam_Detection_Pipeline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aWSKArh_XPShy1zBFxqZM1h72buhyJbb

###  Problem statement

- Create a model for predicting wheather email is spam or ham

**1. Importing Libraries**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
import nltk
import re
import emoji
from collections import Counter
from wordcloud import WordCloud
import pickle
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize,sent_tokenize
from nltk.stem import PorterStemmer,LancasterStemmer,SnowballStemmer,WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import BernoulliNB,MultinomialNB
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
from sklearn.pipeline import Pipeline,make_pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import FunctionTransformer,PowerTransformer,StandardScaler,LabelEncoder

"""**2. Data Collection**"""

data = pd.read_csv(r"/content/drive/MyDrive/spam.csv", encoding= 'latin-1', usecols = ["v1","v2"])

df = data.copy()

df.head()

df.shape

df.info()

df.isnull().sum()

df.duplicated().sum()

df.drop_duplicates(keep= "first", inplace= True)

df.duplicated().sum()

df['v1'].value_counts()

sns.countplot(data= df, x= 'v1')
plt.show()

feature_var = df['v2']

feature_var.head()

def EDA_1(data):

    case = " ".join(data).islower()
    html = data.apply(lambda x:True if re.search("<.+?>",x) else False).sum()
    url = data.apply(lambda x:True if re.search("http[s]?://.+? +",x) else False).sum()
    unwanted = data.apply(lambda x:True if re.search("[]()*\-.:,@#$%&^!?/0-9']",x) else False).sum()

    count = 0
    for sen in data:
        for char in sen.split():
            if emoji.is_emoji(char):
                count += 1

    if case == False:
        print("Contains non lowercase letters")
    else:
        print('All are in lowercase letters')

    if html > 0:
        print("Contains html tags")
    else:
        print('No html tags')

    if url > 0:
        print("Contains urls")
    else:
        print('No urls')

    if unwanted > 0:
        print("Contains unwanted characters")
    else:
        print('No unwanted characters')

    if count > 0:
        print('Contains Emoji')
    else:
        print('No Emoji')

EDA_1(feature_var)

class_var = df['v1']

class_var.head()

class_var_encoding = LabelEncoder()

class_var = class_var_encoding.fit_transform(class_var)

class_var = pd.Series(class_var)

class_var.head()

x_train,x_test,y_train,y_test = train_test_split(feature_var, class_var, test_size= 0.2, random_state= 7, stratify= class_var)

x_train,x_cv,y_train,y_cv = train_test_split(x_train, y_train, test_size= 0.2, random_state= 1, stratify= y_train)

x_train.head()

"""**4. Pre-Processing**"""

def emoji_remove(x):

    x = x.apply(lambda x : emoji.demojize(x))

    return x

def lowercase(x):

     return x.str.lower()

def html_tags(x):

    x = x.apply(lambda x:re.sub("<.+?>"," ",x))

    return x

def urls(x):

    x = x.apply(lambda x : re.sub("https[s]?://.+? +"," ",x))

    return x

def unwanted_characters(x):

    x = x.apply(lambda x : re.sub("[]()*\-.:,@#$%&^!?/0-9']"," ",x))

    return x

def lemmatization(x):

    list_stp = stopwords.words("english")
    wl = WordNetLemmatizer()

    def lemmatize_text(text):

        words = word_tokenize(text)
        lemmatized_words = [wl.lemmatize(word, pos="v") for word in words if word not in list_stp]

        return " ".join(lemmatized_words)

    return x.apply(lemmatize_text)

preprocesser_pipe = Pipeline([('Lowercase', FunctionTransformer(lowercase)),
                              ('Html_Tags', FunctionTransformer(html_tags)),
                              ('Urls', FunctionTransformer(urls)),
                              ("Emoji's", FunctionTransformer(emoji_remove)),
                              ('Unwanted Characters', FunctionTransformer(unwanted_characters)),
                              ('Lemmatization',FunctionTransformer(lemmatization))])

"""**5. EDA After Pre-Processing**"""

x_train = preprocesser_pipe.fit_transform(x_train)

eda_feature_var = pd.DataFrame(list(x_train), columns= ['Eamil'])

eda_class_var = pd.DataFrame(list(y_train), columns= ['spam or ham'])

eda_df = pd.concat([eda_feature_var, eda_class_var], axis= 1)

eda_df.head()

eda_df.info()

group_df = eda_df.groupby('spam or ham')

ham_data = group_df.get_group(0)['Eamil']

spam_data = group_df.get_group(1)['Eamil']

ham_data_words = ' '.join(ham_data).split()

spam_data_words = ' '.join(spam_data).split()

words = []
freq = []
for i,j in Counter(ham_data_words).most_common(7):
    words.append(i)
    freq.append(j)
sns.barplot(x= words, y= freq)
plt.xlabel('Words')
plt.ylabel('Frequency of word')
plt.title('Top 7 common words')
plt.xticks(rotation = 90)
plt.show()

words = []
freq = []
for i,j in Counter(spam_data_words).most_common(7):
    words.append(i)
    freq.append(j)
sns.barplot(x= words, y= freq)
plt.xlabel('Words')
plt.ylabel('Frequency of word')
plt.title('Top 7 common words')
plt.xticks(rotation = 90)
plt.show()

wc = WordCloud().generate(' '.join(ham_data))
plt.imshow(wc)
plt.show()

wc = WordCloud().generate(' '.join(ham_data))
plt.imshow(wc)
plt.show()

"""**6. Feature Engineering**"""

feature_extraction_pipe = Pipeline([('Pre-Processing',preprocesser_pipe),
                                    ('Vectorization',CountVectorizer())])

final_x_train = feature_extraction_pipe.fit_transform(x_train)

final_x_cv = feature_extraction_pipe.transform(x_cv)

final_x_test = feature_extraction_pipe.transform(x_test)

pickle.dump(feature_extraction_pipe,open(r"/content/drive/MyDrive/spam_feature_extraction.pkl",'wb'))

"""**7. Training & Evalution**"""

for i in range(1,10):
    multi = MultinomialNB(alpha= i)
    model = multi.fit(final_x_train,y_train)
    print(('*'*18 + ' Alpha - {} ' + '*'*18).format(i))
    print(classification_report(y_train, model.predict(final_x_train)))

for i in range(1,10):
    multi = MultinomialNB(alpha= i)
    model = multi.fit(final_x_train,y_train)
    print(('*'*18 + ' Alpha - {} ' + '*'*18).format(i))
    print(classification_report(y_cv, model.predict(final_x_cv)))

"""**8. Testing**"""

multi = MultinomialNB(alpha= 1)
final_model = multi.fit(final_x_train,y_train)

print(classification_report(y_test, model.predict(final_x_test)))

"""### 9. Deploying"""

pickle.dump(final_model,open(r"/content/drive/MyDrive/spam_model.pkl",'wb'))

m = feature_extraction_pipe.transform(x_test.iloc[[1]])
pred = final_model.predict(m)

if pred == 1:
  print("spam")
else:
  print("ham")

